{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aabda4-8799-44de-a5a4-6c563eb45dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tue Mar  7 19:42:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 23%   32C    P8     9W / 250W |      4MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from broadcaster_nets_2_3 import *\n",
    "\n",
    "from BCMPLayer import *\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch.nn as N\n",
    "\n",
    "import itertools\n",
    "\n",
    "import csv\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "from layer2Test import netTest2layer\n",
    "from netsGenerator import *\n",
    "from time import perf_counter\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import faiss\n",
    "import faiss.contrib.torch_utils\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_PATH'] = '/modules/apps/cuda/11.3.1' \n",
    "# import pykeops\n",
    "# from pykeops.torch import LazyTensor\n",
    "# # pykeops.clean_pykeops() \n",
    "# pykeops.test_torch_bindings()\n",
    "\n",
    "from guppy import hpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31d049f-5a9f-463f-b44d-d60a20056b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ssahibul_umass_edu/.conda/envs/bc113/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/ssahibul_umass_edu/.local/lib/python3.8/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' for using the GraphGym experiment manager via 'pip install yacs'.\n",
      "  warnings.warn(\"Could not define global config object. Please install \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([19])\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssahibul_umass_edu/.local/lib/python3.8/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "/home/ssahibul_umass_edu/.local/lib/python3.8/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n"
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# for obj in gc.get_objects():\n",
    "#     try:\n",
    "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#             print(type(obj), obj.size())\n",
    "#     except:\n",
    "#         pass\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c315dcb3-8b8e-43cd-a028-c2958516a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cora = dataset[0].to(device)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "bc_assigments, bc_features, counter = kmeansFaissGood(cora.x,k=7,device = device, maxIters=200, centroids=None, res = res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa78911-5e16-4ded-b6ab-60f21f81d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_geometric.utils \n",
    "\n",
    "class BCMPLayer:\n",
    "    def __init__(self,dimWX,dimWZ,dimwalpha,device):\n",
    "        #\n",
    "        self.WX = torch.nn.parameter.Parameter(torch.rand(size=dimWX,device=device),requires_grad=True) #how init?\n",
    "        self.WZ =  torch.nn.parameter.Parameter(torch.rand(size=dimWZ,device=device),requires_grad=True) #how init?\n",
    "        self.Walpha =  torch.nn.parameter.Parameter(torch.rand(size=dimwalpha,device=device),requires_grad=True) #how init?\n",
    "        self.fuse = lambda Xold,Xprim,Zprime,Zalpha : torch.mean(torch.stack((Xold,Xprim,Zprime,Zalpha),dim=2),dim=2)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,x,edge_index,bc_feature,bc_assigment):\n",
    "        A = torch.sparse_coo_tensor(edge_index,torch.ones(edge_index.size(dim=1),device=self.device),(x.size(dim=0),x.size(dim=0)),device=self.device)\n",
    "        Z = bc_feature\n",
    "        bc_assigment_edges = torch.vstack((torch.arange(0,(bc_assigment.size(dim=0)),device=self.device),bc_assigment)) # non symetric\n",
    "        bc_assigment_edges_2 =  torch.vstack((bc_assigment, torch.arange(0,(bc_assigment.size(dim=0)),device=self.device))) # reverse symetric\n",
    "        bc_assigment_edges = torch.cat((bc_assigment_edges,bc_assigment_edges_2),dim=1) # symetric\n",
    "        print(bc_assigment_edges.shape)\n",
    "        B =  torch.sparse_coo_tensor(bc_assigment_edges,torch.ones(bc_assigment_edges.size(dim=1),device=self.device),(x.size(dim=0),bc_feature.size(dim=0)),device=self.device) #rectangle\n",
    "        #square b                             \n",
    "        # bc_assigment_edges = torch.tensor([torch.range(0,(bc_feature.size(dim=0))-1,device=self.device),torch.add(bc_assigment+x.size(dim=0))]device=self.device) # non symetric\n",
    "        # # bc_assigment_edges_2 = torch.tensor([torch.add(bc_assigment+x.size(dim=0)),torch.range(0,(bc_feature.size(dim=0))-1,device=self.device)],device=self.device) # reverse symetric\n",
    "        # # bc_assigment_edges = torch.cat((bc_assigment_edges,bc_assigment_edges_2),dim=1) # symetric\n",
    "        # b =  torch.sparse_coo_tensor(bc_assigment_edges,torch.ones(bc_assigment_edges.size(dim=0)),(x.size(dim=0)+bc_feature.size(dim=0),x.size(dim=0)+bc_feature.size(dim=0),device=self.device) #square\n",
    "                               \n",
    "        Xprime = x@self.WX\n",
    "        Xold = Xprime\n",
    "        Zprime = Z@self.WZ\n",
    "        Zalpha = Z@self.Walpha\n",
    "        Aselfloop = torch.eye(A.size(dim=0),device=self.device)+A#no longer sparse\n",
    "        Bselfloop = torch.eye(n=B.size(dim=0),m=B.size(dim=1),device=self.device)+B#no longer sparse\n",
    "        print(Aselfloop.is_sparse)\n",
    "        D = torch.diag(torch.sum(Aselfloop,dim=0)) # include self-loop\n",
    "        Drow = torch.diag(torch.sum(Bselfloop,dim=1)) # include self-loop\n",
    "        Dcol = torch.diag(torch.sum(torch.transpose(Bselfloop,0,1),dim=1)) # include self-loop \n",
    "        Ahat = torch.pow(D,.5)@(Aselfloop)@torch.pow(D,.5)\n",
    "        Bhat = torch.pow(Drow,.5)@(Bselfloop)@torch.pow(Dcol,.5)\n",
    "        print(Ahat.is_sparse)\n",
    "        print(Xprime.is_sparse)\n",
    "        Ahat = Ahat.to_sparse()\n",
    "        Bhat = Bhat.to_sparse()\n",
    "        print(Ahat.is_sparse)\n",
    "        # Xprime = torch_geometric.utils.spmm(Ahat.to_sparse(),Xprime,\"sum\")\n",
    "        # Zprime = torch_geometric.utils.spmm(Bhat.to_sparse(),Zprime,\"sum\")\n",
    "        # Zalpha = torch_geometric.utils.spmm(Ahat.to_sparse(),spmm(Bhat,Zalpha,\"sum\"),\"sum\")\n",
    "        Xprime = torch.sparse.mm(Ahat,Xprime)\n",
    "        Zprime = torch.sparse.mm(Bhat,Zprime)\n",
    "        Zalpha = torch.sparse.mm(Ahat,torch.sparse.mm(Bhat,Zalpha))\n",
    "        return self.fuse(Xold,Xprime,Zprime,Zalpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f71d1d-692c-438c-a07a-9185e5a7038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5416])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size is inconsistent with indices: for dim 1, size is 7 but found index 2707",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m layer \u001b[38;5;241m=\u001b[39m BCMPLayer((cora\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m64\u001b[39m),(cora\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m64\u001b[39m),(cora\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m64\u001b[39m),device)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbc_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbc_assigments\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mBCMPLayer.forward\u001b[0;34m(self, x, edge_index, bc_feature, bc_assigment)\u001b[0m\n\u001b[1;32m     21\u001b[0m bc_assigment_edges \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((bc_assigment_edges,bc_assigment_edges_2),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# symetric\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(bc_assigment_edges\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 23\u001b[0m B \u001b[38;5;241m=\u001b[39m  \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_coo_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc_assigment_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc_assigment_edges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbc_feature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#rectangle\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#square b                             \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# bc_assigment_edges = torch.tensor([torch.range(0,(bc_feature.size(dim=0))-1,device=self.device),torch.add(bc_assigment+x.size(dim=0))]device=self.device) # non symetric\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# # bc_assigment_edges_2 = torch.tensor([torch.add(bc_assigment+x.size(dim=0)),torch.range(0,(bc_feature.size(dim=0))-1,device=self.device)],device=self.device) # reverse symetric\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# # bc_assigment_edges = torch.cat((bc_assigment_edges,bc_assigment_edges_2),dim=1) # symetric\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# b =  torch.sparse_coo_tensor(bc_assigment_edges,torch.ones(bc_assigment_edges.size(dim=0)),(x.size(dim=0)+bc_feature.size(dim=0),x.size(dim=0)+bc_feature.size(dim=0),device=self.device) #square\u001b[39;00m\n\u001b[1;32m     30\u001b[0m Xprime \u001b[38;5;241m=\u001b[39m x\u001b[38;5;129m@self\u001b[39m\u001b[38;5;241m.\u001b[39mWX\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size is inconsistent with indices: for dim 1, size is 7 but found index 2707"
     ]
    }
   ],
   "source": [
    "layer = BCMPLayer((cora.x.size(dim=1),64),(cora.x.size(dim=1),64),(cora.x.size(dim=1),64),device)\n",
    "print(layer.forward(cora.x,cora.edge_index,bc_features,bc_assigments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7aeee19-a9e1-49c6-bd92-88e64de65765",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX = torch.nn.parameter.Parameter(torch.rand(size=(2708,1433),device=device),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15aad5af-63f0-4f5b-8a06-d380c657950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0570, 0.4245, 0.9248,  ..., 0.2860, 0.0191, 0.8456],\n",
      "        [0.1581, 0.4196, 0.8478,  ..., 0.0681, 0.3777, 0.7560],\n",
      "        [0.8978, 0.3547, 0.6222,  ..., 0.3651, 0.6225, 0.3101],\n",
      "        ...,\n",
      "        [0.8934, 0.0462, 0.3747,  ..., 0.1905, 0.5473, 0.3685],\n",
      "        [0.7146, 0.5697, 0.1196,  ..., 0.7565, 0.9123, 0.8578],\n",
      "        [0.8174, 0.1207, 0.7471,  ..., 0.2825, 0.0780, 0.7247]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(WX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-bc113)",
   "language": "python",
   "name": "conda-env-.conda-bc113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
